{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalized Practice Quizzes\n",
    "### *Leveraging Collaborative Filtering for Personalized Practice in Computer-Based Assessments*\n",
    "This notebook builds multiple **recommender systems** based on **six different collaboartive filtering (CF) techniques** and compares these models against each other based on their performance on a dataset containing student performance data (i.e., a dataset containing the scores of students on assessment questions). Our chosen CF models include **three latent factor-based models** (Singular Value Decomposition, Singular Value Decomposition Plus, Non-Negative Matrix Factorization) and **three neighborhood-based models** (k-Nearest Neighbors, k-Nearest Neighbors with Means, k-Nearest Neighbors with Z-Scores).\n",
    "\n",
    "To evaluate whether CF-based recommender systems can effectively predict students' performance scores on new, unseen questions based on their past performance, this notebook conducts **[Dietterich's 5x2 CV paired t-test](https://pubmed.ncbi.nlm.nih.gov/9744903/)** on each model. The average **Mean Absolute Error (MAE)** and **Root Mean Squared Error (RMSE)** are first reported and then a **paired t-test** is performed to assess whether the performance difference between each CF model and an average-based baseline model is statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Set dataset-specific variables\n",
    "After setting the following dataset-specific variables, you should be able to run this notebook without any additional changes.\n",
    "\n",
    "**NOTE**: This notebook assumes that the student performance dataset is stored as CSV file with one column for the (anonymized) student ID, one column for the question ID, and one column for the **normalized** score (i.e., a score falling between 0.0 and 1.0). If your dataset does not follow these specifications, you will also need to change the implementation of the ``load_and_preprocess_data`` function accordingly based on the shape of your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill in the values for these variables before running the remaining cells of this notebook!\n",
    "\n",
    "# a string that specifies the path to the performance dataset from the current directory\n",
    "dataset_path = ''\n",
    "# a string that specifies the NAME of the column containing the (anonymized) student IDs \n",
    "student_id_col_name = ''\n",
    "# a string that specifies the NAME of the column containing the question IDs \n",
    "question_id_col_name = ''\n",
    "# a string that specifies the NAME of the column containing the normalized performance scores\n",
    "score_col_name = ''\n",
    "\n",
    "# set to False if you want to disable status messages during model evaluation\n",
    "include_status_messages = True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Import packages\n",
    "We use the [Surprise](https://surpriselib.com/) package, a Python scikit for building and analyzing CF-based recommender systems, to build and evaluate our CF models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import t\n",
    "from surprise import Reader, Dataset\n",
    "from surprise import AlgoBase, SVD, SVDpp, NMF, KNNBasic, KNNWithMeans, KNNWithZScore\n",
    "from surprise.model_selection import KFold, cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Load and preprocess raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(path):\n",
    "    \"\"\"\n",
    "    Loads the performance dataset from the CSV file located at path.\n",
    "    \n",
    "    @param path: path to the performance dataset from the current directory\n",
    "    @return data_df: Pandas dataframe containing the loaded performance dataset\n",
    "    @return data_wrapped: Surprise dataframe containing the loaded performance dataset\n",
    "    \"\"\"\n",
    "    data_df = pd.read_csv(path, keep_default_na=False)\n",
    "    \n",
    "    # rename columns to match the names expected by the functions in the Surprise package\n",
    "    data_df = data_df.rename(\n",
    "        columns={question_id_col_name:'itemID', student_id_col_name:'userID', score_col_name:'rating'})\n",
    "    data_df = data_df[['itemID', 'userID', 'rating']]\n",
    "    data_df['rating'] = pd.to_numeric(data_df['rating']).fillna(0)\n",
    "    \n",
    "    # functions in the Suprise package require the data to be wrapped by a Surprise wrapper class\n",
    "    reader = Reader(rating_scale=(0.0, 1.0))\n",
    "    data_wrapped = Dataset.load_from_df(data_df[['userID', 'itemID', 'rating']], reader)\n",
    "    \n",
    "    return data_df, data_wrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df, data_wrapped = load_and_preprocess_data(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify that the dataset was loaded properly\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report the number of students and questions in the dataset\n",
    "num_students = len(set(data_df['userID']))\n",
    "num_questions = len(set(data_df['itemID']))\n",
    "print('Number of distinct  students in dataset: %d' % num_students)\n",
    "print('Number of distinct questions in dataset: %d' % num_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Implement a baseline model\n",
    "We compare the performance of our CF models against an **average-based baseline model**, a standard benchmark in recommender system evaluations. For a given student and a new question, the baseline model predicts a performance score based on the average of three means: the overall mean score, the mean score of the student, and the mean score of the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AvgBaseline(AlgoBase):\n",
    "    def __init__(self):\n",
    "        AlgoBase.__init__(self, random_state=0)\n",
    "\n",
    "    def fit(self, trainset):\n",
    "        \"\"\"\n",
    "        Fits the average-based model to the provided training set.\n",
    "\n",
    "        @param trainset: training set (wrapped by Surprise wrapper class) \n",
    "        \"\"\"\n",
    "        AlgoBase.fit(self, trainset)\n",
    "        self.avg_rating = np.mean([r for (_, _, r) in self.trainset.all_ratings()])\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        \"\"\"\n",
    "        Predicts the score of user/student u on item/student i. \n",
    "\n",
    "        @param u: ID of the user/student\n",
    "        @param i: ID of the item/question\n",
    "        @return: the predicted score of user/student u on item/student i\n",
    "        \"\"\"\n",
    "        sum_means = self.avg_rating \n",
    "        div = 1\n",
    "        if self.trainset.knows_user(u):\n",
    "            sum_means += np.mean([r for (_, r) in self.trainset.ur[u]])\n",
    "            div += 1\n",
    "        if self.trainset.knows_item(i):\n",
    "            sum_means += np.mean([r for (_, r) in self.trainset.ir[i]])\n",
    "            div += 1\n",
    "\n",
    "        return sum_means / div"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Build and evaluate models\n",
    "We follow [Dietterich's 5x2 CV technique](https://pubmed.ncbi.nlm.nih.gov/9744903/) to evaluate each of our models across the two benchmarking metrics of Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE). This results in 5 iterations of 2-fold cross-validation for each model, giving us a total of 10 trials per model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['baseline', 'svd', 'svdpp', 'nmf', 'knn_basic', 'knn_means', 'knn_zscore']\n",
    "num_iterations = 5 # set to 5 for Dietterich's 5x2 CV test \n",
    "num_splits = 2     # set to 2 for Dietterich's 5x2 CV test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_wrapped, kfold):\n",
    "    \"\"\"\n",
    "    Performs cross-validation on the specified model instance using MAE and RMSE as its measures.\n",
    "    \n",
    "    @param model: model instance to which cross-validation is applied\n",
    "    @param data_wrapped: dataset that will be used for cross-validation (wrapped by a Surprise class)\n",
    "    @param kfold: KFold object specifying the number of splits to use for cross-validation\n",
    "    @return results: dictionary containing the MAE and RMSE values for each testset from cross-validation.\n",
    "    \"\"\"\n",
    "    cv_results = cross_validate(model, data_wrapped, measures=['MAE', 'RMSE'], cv=kfold, verbose=include_status_messages)\n",
    "    results = {\n",
    "        'MAE': cv_results['test_mae'],\n",
    "        'RMSE': cv_results['test_rmse']\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# evalute all the models using Dietterich's 5x2 CV technique \n",
    "models_results = {\n",
    "    'baseline': [],\n",
    "    'svd': [],\n",
    "    'svdpp': [],\n",
    "    'nmf': [],\n",
    "    'knn_basic': [],\n",
    "    'knn_means': [],\n",
    "    'knn_zscore': []\n",
    "}\n",
    "for i in range(num_iterations):\n",
    "    print('\\n** ITERATION ROUND %d **' %(i+1))\n",
    "    random.seed(i)                                                                    \n",
    "    np.random.seed(i)\n",
    "    kfold = KFold(n_splits=num_splits, random_state=i)\n",
    "    \n",
    "    models_results['baseline'].append(evaluate_model(AvgBaseline(), data_wrapped, kfold))\n",
    "    models_results['svd'].append(evaluate_model(SVD(random_state=i), data_wrapped, kfold))\n",
    "    models_results['svdpp'].append(evaluate_model(SVDpp(random_state=i), data_wrapped, kfold))\n",
    "    models_results['nmf'].append(evaluate_model(NMF(random_state=i), data_wrapped, kfold))\n",
    "    models_results['knn_basic'].append(evaluate_model(KNNBasic(), data_wrapped, kfold))\n",
    "    models_results['knn_means'].append(evaluate_model(KNNWithMeans(), data_wrapped, kfold))\n",
    "    models_results['knn_zscore'].append(evaluate_model(KNNWithZScore(), data_wrapped, kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Compare models\n",
    "We report the mean MAE and RMSE values (from the 10 trials of the 5x2 CV process) for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_means(models_results, model_name):\n",
    "    \"\"\"\n",
    "    Computes and prints the mean MAE and RMSE values (from the 10 trials of the 5x2 CV process) for each model.\n",
    "    \n",
    "    @param model_results: the dictionary generated in the evaluation step containing the MAE and RMSE values\n",
    "                          for all the models across all iterations and splits\n",
    "    @param model_name: name of the model for which the mean MAE and RMSE will be computed\n",
    "    \"\"\"\n",
    "    print(f'\\n** RESULTS FOR {model_name.upper()} **')\n",
    "    for metric_name in ['MAE', 'RMSE']:\n",
    "        metric_vals = np.array([])\n",
    "        for i in range(num_iterations):\n",
    "            metric_vals = np.append(metric_vals, models_results[model_name][i][metric_name])\n",
    "        metric_mean = np.mean(metric_vals)\n",
    "        metric_std = np.std(metric_vals)\n",
    "        print(f'Mean of {metric_name}: {metric_mean} +/- {metric_std}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    compute_means(models_results, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the statistical significance of the performance differences between each CF model and the average-based baseline, we perform a paired t-test using the 5x2 CV approach (as outlined in [Dietterich's paper](https://pubmed.ncbi.nlm.nih.gov/9744903/) under **Section 3.5 - The 5x2cv paired t-test**), with the assumption that the t-stat approximately follows a t-distribution with 5 degrees of freedom and a null hypothesis that both models 1 and 2 have equal performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paired_ttest(models_results, model1_name, model2_name):\n",
    "    \"\"\"\n",
    "    Computes and prints the p-values resulting form the comparison of models 1 and 2 across MAE and RMSE.\n",
    "    \n",
    "    @param model_results: the dictionary generated in the evaluation step containing the MAE and RMSE values\n",
    "                          for all the models across all iterations and splits\n",
    "    @param model1_name: name of the 1st model to be used in the paired t-test\n",
    "    @param model2_name: name of the 2nd model to be used in the paired t-test\n",
    "    \"\"\"\n",
    "    print(f'\\n** RESULTS FOR COMPARING {model1_name.upper()} AND {model2_name.upper()} **')\n",
    "    for metric_name in ['MAE','RMSE']:\n",
    "        perf_diff_var_sum = 0\n",
    "        for i in range(num_iterations):\n",
    "            perf_diff = models_results[model1_name][i][metric_name] - models_results[model2_name][i][metric_name]\n",
    "            perf_diff_mean = np.mean(perf_diff)\n",
    "            perf_diff_var = np.sum((perf_diff - perf_diff_mean)**2)\n",
    "            perf_diff_var_sum += perf_diff_var\n",
    "\n",
    "        perf_diff_first = models_results[model1_name][0][metric_name] - models_results[model2_name][0][metric_name]\n",
    "        t_stat = perf_diff_first[0] / np.sqrt(1/num_iterations*perf_diff_var_sum)\n",
    "        p_val = 2*(1 - t.cdf(abs(t_stat), num_iterations))\n",
    "\n",
    "        print(f'P-value based on {metric_name}: {p_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in model_names:\n",
    "    if model_name != 'baseline':\n",
    "        paired_ttest(models_results, model_name, 'baseline')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
